{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer1:\n",
      "{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}\n",
      "1\n",
      "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad'])\n",
      "Optimizer2:\n",
      "{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}\n",
      "1\n",
      "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad'])\n",
      "Optimizer3:\n",
      "{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}\n",
      "2\n",
      "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad'])\n"
     ]
    }
   ],
   "source": [
    "class lr_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lr_test, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model1 = lr_test()\n",
    "model2 = lr_test()\n",
    "\n",
    "initial_lr = 1e-2\n",
    "\n",
    "# 只有一个模型的参数优化\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr = initial_lr)\n",
    "# defaults: dict\n",
    "# state: dict\n",
    "# param_groups: List[dict]\n",
    "print('Optimizer1:')\n",
    "print(optimizer1.defaults)\n",
    "# print(optimizer1.state)\n",
    "print(len(optimizer1.param_groups))\n",
    "print(optimizer1.param_groups[0].keys())\n",
    "\n",
    "# 两个模型的参数优化一起\n",
    "optimizer2 = torch.optim.Adam([*model2.parameters(), *model1.parameters()], lr=initial_lr)\n",
    "print('Optimizer2:')\n",
    "print(optimizer2.defaults)\n",
    "print(len(optimizer2.param_groups))\n",
    "print(optimizer2.param_groups[0].keys())\n",
    "\n",
    "# 分别优化两个不同的模型\n",
    "\n",
    "# 为不同的模型设置不同的学习率\n",
    "optimizer3 = torch.optim.Adam([{'params':model1.parameters(), 'lr': 1e-2}, {'params':model2.parameters(), 'lr' : 1e-3}])\n",
    "# 为模型设置相同的学习率\n",
    "# optimizer3 = torch.optim.Adam([{'params':model1.parameters()}, {'params':model2.parameters()}], lr=initial_lr)\n",
    "print('Optimizer3:')\n",
    "print(optimizer3.defaults)\n",
    "print(len(optimizer3.param_groups))\n",
    "print(optimizer3.param_groups[0].keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LambdaLR\n",
    "学习率乘以系数lambda；也可以表示没有确定形式的lambda函数的学习率"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的learning_rate为0.01\n",
      "Stable情况下：第1次训练的学习率为0.01\n",
      "Flexible情况下：第1次训练的学习率为0.01\n",
      "Stable情况下：第2次训练的学习率为0.009000000000000001\n",
      "Flexible情况下：第2次训练的学习率为0.005\n",
      "Stable情况下：第3次训练的学习率为0.008100000000000001\n",
      "Flexible情况下：第3次训练的学习率为0.003333333333333333\n",
      "Stable情况下：第4次训练的学习率为0.007290000000000001\n",
      "Flexible情况下：第4次训练的学习率为0.0025\n",
      "Stable情况下：第5次训练的学习率为0.006561\n",
      "Flexible情况下：第5次训练的学习率为0.002\n",
      "Stable情况下：第6次训练的学习率为0.005904900000000001\n",
      "Flexible情况下：第6次训练的学习率为0.0016666666666666666\n",
      "Stable情况下：第7次训练的学习率为0.00531441\n",
      "Flexible情况下：第7次训练的学习率为0.0014285714285714286\n",
      "Stable情况下：第8次训练的学习率为0.004782969000000001\n",
      "Flexible情况下：第8次训练的学习率为0.00125\n",
      "Stable情况下：第9次训练的学习率为0.004304672100000001\n",
      "Flexible情况下：第9次训练的学习率为0.0011111111111111111\n",
      "Stable情况下：第10次训练的学习率为0.003874204890000001\n",
      "Flexible情况下：第10次训练的学习率为0.001\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Lambdalr_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lambdalr_test, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "writer = SummaryWriter(log_dir='LR_Logs')\n",
    "\n",
    "initial_lr = 1e-2\n",
    "model_Lambda = Lambdalr_test()\n",
    "optimizer_Stable_Lambda = torch.optim.SGD(params=model_Lambda.parameters(), lr=initial_lr)\n",
    "optimizer_Flexible_Lambda = torch.optim.SGD(params=model_Lambda.parameters(), lr=initial_lr)\n",
    "scheduler_Flexible_Lambda = LambdaLR(optimizer = optimizer_Flexible_Lambda, lr_lambda=lambda epoch: 1/(epoch+1))    # 动态变化的Lambda\n",
    "scheduler_Stable_Lambda = LambdaLR(optimizer = optimizer_Stable_Lambda, lr_lambda=lambda epoch: 0.9**epoch)    # 固定系数的Lambda\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "print(f'初始化的learning_rate为{initial_lr}')\n",
    "for i in range(epoch):\n",
    "    # train\n",
    "\n",
    "    # Optimize\n",
    "    optimizer_Stable_Lambda.zero_grad()\n",
    "    optimizer_Stable_Lambda.step()\n",
    "    optimizer_Flexible_Lambda.zero_grad()\n",
    "    optimizer_Flexible_Lambda.step()\n",
    "    print(f'Stable情况下：第{i+1}次训练的学习率为{optimizer_Stable_Lambda.param_groups[0][\"lr\"]}')\n",
    "    print(f'Flexible情况下：第{i+1}次训练的学习率为{optimizer_Flexible_Lambda.param_groups[0][\"lr\"]}')\n",
    "    writer.add_scalar(tag='Lambda_Stable_LR', scalar_value=optimizer_Stable_Lambda.param_groups[0]['lr'], global_step=i)\n",
    "    writer.add_scalar(tag='Lambda_Flexible_LR', scalar_value=optimizer_Flexible_Lambda.param_groups[0]['lr'], global_step=i)\n",
    "    scheduler_Flexible_Lambda.step()\n",
    "    scheduler_Stable_Lambda.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img_1.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# StepLR\n",
    "在固定步长的位置更新学习率"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的learning_rate为0.01\n",
      "Step情况下：第1次训练的学习率为0.01\n",
      "Step情况下：第2次训练的学习率为0.01\n",
      "Step情况下：第3次训练的学习率为0.01\n",
      "Step情况下：第4次训练的学习率为0.01\n",
      "Step情况下：第5次训练的学习率为0.01\n",
      "Step情况下：第6次训练的学习率为0.001\n",
      "Step情况下：第7次训练的学习率为0.001\n",
      "Step情况下：第8次训练的学习率为0.001\n",
      "Step情况下：第9次训练的学习率为0.001\n",
      "Step情况下：第10次训练的学习率为0.001\n",
      "Step情况下：第11次训练的学习率为0.0001\n",
      "Step情况下：第12次训练的学习率为0.0001\n",
      "Step情况下：第13次训练的学习率为0.0001\n",
      "Step情况下：第14次训练的学习率为0.0001\n",
      "Step情况下：第15次训练的学习率为0.0001\n",
      "Step情况下：第16次训练的学习率为1e-05\n",
      "Step情况下：第17次训练的学习率为1e-05\n",
      "Step情况下：第18次训练的学习率为1e-05\n",
      "Step情况下：第19次训练的学习率为1e-05\n",
      "Step情况下：第20次训练的学习率为1e-05\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Steplr_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Steplr_test, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "writer = SummaryWriter(log_dir='LR_Logs')\n",
    "\n",
    "initial_lr = 1e-2\n",
    "model_Step = Steplr_test()\n",
    "optimizer_Step = torch.optim.SGD(params=model_Step.parameters(), lr=initial_lr)\n",
    "scheduler_Step = StepLR(optimizer = optimizer_Step, step_size=5, gamma=0.1)\n",
    "\n",
    "epoch = 20\n",
    "\n",
    "print(f'初始化的learning_rate为{initial_lr}')\n",
    "for i in range(epoch):\n",
    "    # train\n",
    "\n",
    "    # Optimize\n",
    "    optimizer_Step.zero_grad()\n",
    "    optimizer_Step.step()\n",
    "    print(f'Step情况下：第{i+1}次训练的学习率为{optimizer_Step.param_groups[0][\"lr\"]}')\n",
    "    writer.add_scalar(tag='Step_LR', scalar_value=optimizer_Step.param_groups[0]['lr'], global_step=i)\n",
    "    scheduler_Step.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MultiStep\n",
    "在指定的MileStone位置进行改变学习率"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的learning_rate为0.01\n",
      "MultiStep情况下：第1次训练的学习率为0.01\n",
      "MultiStep情况下：第2次训练的学习率为0.01\n",
      "MultiStep情况下：第3次训练的学习率为0.0005\n",
      "MultiStep情况下：第4次训练的学习率为0.0005\n",
      "MultiStep情况下：第5次训练的学习率为0.0005\n",
      "MultiStep情况下：第6次训练的学习率为0.0005\n",
      "MultiStep情况下：第7次训练的学习率为0.0005\n",
      "MultiStep情况下：第8次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第9次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第10次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第11次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第12次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第13次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第14次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第15次训练的学习率为2.5e-05\n",
      "MultiStep情况下：第16次训练的学习率为1.25e-06\n",
      "MultiStep情况下：第17次训练的学习率为1.25e-06\n",
      "MultiStep情况下：第18次训练的学习率为1.25e-06\n",
      "MultiStep情况下：第19次训练的学习率为1.25e-06\n",
      "MultiStep情况下：第20次训练的学习率为1.25e-06\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiSteplr_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiSteplr_test, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "writer = SummaryWriter(log_dir='LR_Logs')\n",
    "epoch = 20\n",
    "\n",
    "initial_lr = 1e-2\n",
    "model_MultiStep = MultiSteplr_test()\n",
    "optimizer_MultiStep = torch.optim.SGD(params=model_MultiStep.parameters(), lr=initial_lr)\n",
    "scheduler_MultiStep = MultiStepLR(optimizer=optimizer_MultiStep, milestones=[2,7,15], gamma=0.05)\n",
    "\n",
    "print(f'初始化的learning_rate为{initial_lr}')\n",
    "for i in range(epoch):\n",
    "    # train\n",
    "\n",
    "    # Optimize\n",
    "    optimizer_MultiStep.zero_grad()\n",
    "    optimizer_MultiStep.step()\n",
    "    print(f'MultiStep情况下：第{i+1}次训练的学习率为{optimizer_MultiStep.param_groups[0][\"lr\"]}')\n",
    "    writer.add_scalar(tag='MultiStep_LR', scalar_value=optimizer_MultiStep.param_groups[0]['lr'], global_step=i)\n",
    "    scheduler_MultiStep.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img_2.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CosineAnnealingLR\n",
    "T_max (int)：对于周期函数cosine，T_max就是这个周期的一半。\n",
    "\n",
    "eta_min (float)：最小的学习率，默认值为0。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的learning_rate为0.1\n",
      "CosineAnnealing情况下：第1次训练的学习率为0.1\n",
      "CosineAnnealing情况下：第2次训练的学习率为0.08535533905932738\n",
      "CosineAnnealing情况下：第3次训练的学习率为0.05\n",
      "CosineAnnealing情况下：第4次训练的学习率为0.014644660940672627\n",
      "CosineAnnealing情况下：第5次训练的学习率为0.0\n",
      "CosineAnnealing情况下：第6次训练的学习率为0.014644660940672622\n",
      "CosineAnnealing情况下：第7次训练的学习率为0.05000000000000001\n",
      "CosineAnnealing情况下：第8次训练的学习率为0.0853553390593274\n",
      "CosineAnnealing情况下：第9次训练的学习率为0.10000000000000003\n",
      "CosineAnnealing情况下：第10次训练的学习率为0.0853553390593274\n",
      "CosineAnnealing情况下：第11次训练的学习率为0.05000000000000003\n",
      "CosineAnnealing情况下：第12次训练的学习率为0.014644660940672672\n",
      "CosineAnnealing情况下：第13次训练的学习率为0.0\n",
      "CosineAnnealing情况下：第14次训练的学习率为0.014644660940672622\n",
      "CosineAnnealing情况下：第15次训练的学习率为0.04999999999999992\n",
      "CosineAnnealing情况下：第16次训练的学习率为0.08535533905932723\n",
      "CosineAnnealing情况下：第17次训练的学习率为0.09999999999999988\n",
      "CosineAnnealing情况下：第18次训练的学习率为0.08535533905932725\n",
      "CosineAnnealing情况下：第19次训练的学习率为0.04999999999999996\n",
      "CosineAnnealing情况下：第20次训练的学习率为0.01464466094067266\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CosinesAnnealinglr_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosinesAnnealinglr_test, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "writer = SummaryWriter(log_dir='LR_Logs')\n",
    "epoch = 20\n",
    "\n",
    "initial_lr = 1e-1\n",
    "model_Cosine = CosinesAnnealinglr_test()\n",
    "optimizer_Cosine = torch.optim.SGD(params=model_Cosine.parameters(), lr=initial_lr)\n",
    "scheduler_Cosine = CosineAnnealingLR(optimizer=optimizer_Cosine, T_max=4, eta_min=0)\n",
    "\n",
    "print(f'初始化的learning_rate为{initial_lr}')\n",
    "for i in range(epoch):\n",
    "    # train\n",
    "\n",
    "    # Optimize\n",
    "    optimizer_Cosine.zero_grad()\n",
    "    optimizer_Cosine.step()\n",
    "    print(f'CosineAnnealing情况下：第{i+1}次训练的学习率为{optimizer_Cosine.param_groups[0][\"lr\"]}')\n",
    "    writer.add_scalar(tag='CosineAnnealing_LR', scalar_value=optimizer_Cosine.param_groups[0]['lr'], global_step=i)\n",
    "    scheduler_Cosine.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img_3.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ReduceLROnPlateau\n",
    "mode (str) – One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: ‘min’.\n",
    "\n",
    "factor (float) – Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.\n",
    "\n",
    "patience (int) – Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then. Default: 10.\n",
    "\n",
    "threshold (float) – Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.\n",
    "\n",
    "threshold_mode (str) – One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in ‘max’ mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: ‘rel’.\n",
    "\n",
    "cooldown (int) – Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0.\n",
    "\n",
    "min_lr (float or list) – A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0.\n",
    "\n",
    "eps (float) – Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8.\n",
    "verbose (bool) – If True, prints a message to stdout for each update. Default: False."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的learning_rate为0.001\n",
      "ReduceLROnPlateau情况下：第1次训练的学习率为0.001\n",
      "ReduceLROnPlateau情况下：第2次训练的学习率为0.001\n",
      "ReduceLROnPlateau情况下：第3次训练的学习率为0.001\n",
      "ReduceLROnPlateau情况下：第4次训练的学习率为0.001\n",
      "ReduceLROnPlateau情况下：第5次训练的学习率为5e-05\n",
      "ReduceLROnPlateau情况下：第6次训练的学习率为5e-05\n",
      "ReduceLROnPlateau情况下：第7次训练的学习率为5e-05\n",
      "ReduceLROnPlateau情况下：第8次训练的学习率为2.5e-06\n",
      "ReduceLROnPlateau情况下：第9次训练的学习率为2.5e-06\n",
      "ReduceLROnPlateau情况下：第10次训练的学习率为2.5e-06\n",
      "ReduceLROnPlateau情况下：第11次训练的学习率为1.2500000000000002e-07\n",
      "ReduceLROnPlateau情况下：第12次训练的学习率为1.2500000000000002e-07\n",
      "ReduceLROnPlateau情况下：第13次训练的学习率为1.2500000000000002e-07\n",
      "ReduceLROnPlateau情况下：第14次训练的学习率为6.250000000000001e-09\n",
      "ReduceLROnPlateau情况下：第15次训练的学习率为6.250000000000001e-09\n",
      "ReduceLROnPlateau情况下：第16次训练的学习率为6.250000000000001e-09\n",
      "ReduceLROnPlateau情况下：第17次训练的学习率为6.250000000000001e-09\n",
      "ReduceLROnPlateau情况下：第18次训练的学习率为6.250000000000001e-09\n",
      "ReduceLROnPlateau情况下：第19次训练的学习率为6.250000000000001e-09\n",
      "ReduceLROnPlateau情况下：第20次训练的学习率为6.250000000000001e-09\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ReduceLROnPlateaulr_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceLROnPlateaulr_test, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "writer = SummaryWriter(log_dir='LR_Logs')\n",
    "epoch = 20\n",
    "\n",
    "initial_lr = 1e-3\n",
    "model_Reduce = ReduceLROnPlateaulr_test()\n",
    "optimizer_ReduceLROnPlateau = torch.optim.SGD(params=model_Reduce.parameters(), lr=initial_lr)\n",
    "schedular_ReduceLROnPlateau = ReduceLROnPlateau(optimizer=optimizer_ReduceLROnPlateau, mode='min', factor=0.05, patience=2, verbose=False)\n",
    "\n",
    "print(f'初始化的learning_rate为{initial_lr}')\n",
    "for i in range(epoch):\n",
    "    # train\n",
    "\n",
    "    # Optimize\n",
    "    optimizer_ReduceLROnPlateau.zero_grad()\n",
    "    optimizer_ReduceLROnPlateau.step()\n",
    "    print(f'ReduceLROnPlateau情况下：第{i+1}次训练的学习率为{optimizer_ReduceLROnPlateau.param_groups[0][\"lr\"]}')\n",
    "    writer.add_scalar(tag='ReduceLROnPlateau_LR', scalar_value=optimizer_ReduceLROnPlateau.param_groups[0]['lr'], global_step=i)\n",
    "    schedular_ReduceLROnPlateau.step(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}